{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akinahomwabella/FairXAI/blob/main/fair_xai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eht6UVP-toR"
      },
      "outputs": [],
      "source": [
        "#Import and load libraries\n",
        "!pip install lime\n",
        "!pip install fairlearn\n",
        "!pip install transformers accelerate\n",
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tf_keras\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, DemographicParity, GridSearch\n",
        "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import shap\n",
        "import random\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import torch\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s-wUV74-w3D"
      },
      "outputs": [],
      "source": [
        "#Mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GOBG24l-0cL"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/NCUR\"\n",
        "if os.path.exists(path):\n",
        "  files=[file for file in os.listdir(path) if not file.startswith('.')]\n",
        "  for file in files:\n",
        "    print(file)\n",
        "else:\n",
        "  print(\"Path doesn't exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO5uCjfk-3iY"
      },
      "outputs": [],
      "source": [
        "# Load each dataset separately\n",
        "#This dataset will be on the COMPAS dataset\n",
        "df_compas = pd.read_csv('/content/drive/MyDrive/NCUR/compas-scores-two-years-violent.csv')\n",
        "df_compas.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBwi8-IQ-6la"
      },
      "outputs": [],
      "source": [
        "#Clean the COMPAS dataset next(this is more info to understand which one are relevant)\n",
        "df_compas.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtaDY8OA_SPf"
      },
      "outputs": [],
      "source": [
        "#More understanding on it to see which one equally skewed\n",
        "df_compas.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heCWzaZT_VWd"
      },
      "outputs": [],
      "source": [
        "#Clean the compas dataset\n",
        "#Removing any column that is not needed\n",
        "columns_to_drop = [\n",
        "    'id', 'name', 'first', 'last', 'dob', 'violent_recid',\n",
        "    'decile_score.1', 'priors_count.1', 'two_year_recid.1',\n",
        "    'screening_date', 'v_screening_date', 'start', 'end', 'days_b_screening_arrest', 'c_days_from_compas', 'r_days_from_compas', 'c_days_b_screening', 'r_days_b_screening',\n",
        "    'c_charge_degree', 'r_charge_degree', 'vr_charge_degree',\n",
        "    'c_case_number', 'r_case_number', 'vr_case_number',\n",
        "    'c_arrest_date', 'c_offense_date', 'r_offense_date', 'vr_offense_date',\n",
        "    'c_jail_in', 'c_jail_out', 'r_jail_in', 'r_jail_out',\n",
        "    'c_charge_desc', 'r_charge_desc', 'vr_charge_desc','compas_screening_date','age_cat','v_type_of_assessment','in_custody','out_custody','score_text','type_of_assessment','v_score_text','v_decile_score','event','is_violent_recid','is_recid',\n",
        "]\n",
        "\n",
        "# Remove whitespace(this helps if there is any space )\n",
        "df_compas.columns = df_compas.columns.str.strip()\n",
        "columns_to_drop = [col.strip() for col in columns_to_drop]\n",
        "\n",
        "\n",
        "df_compas = df_compas.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Drop columns with too many NaNs:\n",
        "df_compas = df_compas.dropna(axis=1, thresh=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh37xbNi_Xj1"
      },
      "outputs": [],
      "source": [
        "#Check whether the compas has duplicate or not\n",
        "df_compas.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X26gBtA9_Z4h"
      },
      "outputs": [],
      "source": [
        "#Drop the duplicates for the compas datsset\n",
        "df_compas = df_compas.drop_duplicates()\n",
        "df_compas.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYnvqvSN_bt9"
      },
      "outputs": [],
      "source": [
        "df_compas.to_csv('cleaned_compas.csv',index=False)\n",
        "df_compas_cleaned = df_compas.copy() # work on this copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pleeUe3_h0H"
      },
      "outputs": [],
      "source": [
        "df_compas_cleaned.info()\n",
        "df_compas_cleaned.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRliM642_jYo"
      },
      "outputs": [],
      "source": [
        "#Encoding the COMPAS dataset next\n",
        "# map the encoding to figure out which method we will use during fairness analysis\n",
        "race_mapping = {\n",
        "    'African-American': 0,\n",
        "    'Asian': 1,\n",
        "    'Caucasian': 2,\n",
        "    'Hispanic': 3,\n",
        "    'Native American': 4,\n",
        "    'Other': 5\n",
        "}\n",
        "sex_mapping = {'Female': 0, 'Male':1 }\n",
        "#Add the encoded program into the dataset\n",
        "df_compas_cleaned['race_encoded'] = df_compas_cleaned['race'].map(race_mapping)\n",
        "df_compas_cleaned['sex_encoded'] = df_compas_cleaned['sex'].map(sex_mapping)\n",
        "\n",
        "\n",
        "\n",
        "#Save a csv with an encoded dataset\n",
        "df_compas_cleaned.to_csv('encoded_compas.csv', index=False)\n",
        "df_compas_encoded = df_compas_cleaned.copy()\n",
        "df_compas_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saOlncBL_lSd"
      },
      "outputs": [],
      "source": [
        "#Check if they are actually encoded or not\n",
        "df_compas_encoded['sex_encoded'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1HO5EhJ_nf4"
      },
      "outputs": [],
      "source": [
        "df_compas_encoded['race_encoded'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Nl1usy_pJ1"
      },
      "outputs": [],
      "source": [
        "#Split the dataset into to train and test split\n",
        "#Perform logisitc regression on compas dataset\n",
        "X_compas=df_compas_encoded.drop(columns=['two_year_recid','race','sex'])\n",
        "Y_compas=df_compas_encoded['two_year_recid']\n",
        "X_train_compas, X_test_compas, Y_train_compas, Y_test_compas = train_test_split(X_compas, Y_compas, test_size=0.2, random_state=42, stratify=Y_compas)\n",
        "model_lr=LogisticRegression(max_iter=1000)\n",
        "model_lr.fit(X_train_compas,Y_train_compas)\n",
        "y_pred_lg=model_lr.predict(X_test_compas)\n",
        "#Evaluate the performance\n",
        "print(\"The performance of Logistic regression \")\n",
        "print(classification_report(Y_test_compas,y_pred_lg))\n",
        "print(confusion_matrix(Y_test_compas,y_pred_lg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG9UtP_gAG8F"
      },
      "outputs": [],
      "source": [
        "#Perform random forest classifier(try another model too to see which one is better)\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train_compas, Y_train_compas)\n",
        "y_pred_rf = model_rf.predict(X_test_compas)\n",
        "print(\"The performance of Random Forest Classifier\")\n",
        "#Evaluate the performance\n",
        "print(classification_report(Y_test_compas,y_pred_rf))\n",
        "print(confusion_matrix(Y_test_compas,y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrg0NThXAJ3o"
      },
      "outputs": [],
      "source": [
        "#Perform fairness analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute fairness metrics for a subgroup\n",
        "def compute_metrics(df, sensitive_column, sensitive_value):\n",
        "\n",
        "    group = df[df[sensitive_column] == sensitive_value]\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    TP = ((group['true'] == 1) & (group['pred'] == 1)).sum()\n",
        "    TN = ((group['true'] == 0) & (group['pred'] == 0)).sum()\n",
        "    FP = ((group['true'] == 0) & (group['pred'] == 1)).sum()\n",
        "    FN = ((group['true'] == 1) & (group['pred'] == 0)).sum()\n",
        "\n",
        "    # Calculate metrics, using conditionals to avoid division by zero\n",
        "    fpr = FP / (FP + TN) if (FP + TN) > 0 else None\n",
        "    fnr = FN / (FN + TP) if (FN + TP) > 0 else None\n",
        "    tnr = TN / (TN + FP) if (TN + FP) > 0 else None\n",
        "    tpr = TP / (TP + FN) if (TP + FN) > 0 else None\n",
        "\n",
        "    return {'FPR': fpr, 'FNR': fnr, 'TNR': tnr, 'TPR': tpr}\n",
        "\n",
        "\n",
        "# Prepare test set copy with sensitive attributes\n",
        "# Assuming X_test_compas is test features and df_compas_cleaned is original sensitive\n",
        "\n",
        "X_test_compas_copy = X_test_compas.copy()\n",
        "X_test_compas_copy['true'] = Y_test_compas\n",
        "X_test_compas_copy['pred'] = y_pred_rf\n",
        "\n",
        "# Re-attach sensitive attributes from the cleaned dataframe\n",
        "X_test_compas_copy['race'] = df_compas_cleaned.loc[X_test_compas.index, 'race']\n",
        "X_test_compas_copy['sex'] = df_compas_cleaned.loc[X_test_compas.index, 'sex']\n",
        "\n",
        "#Compute metrics for each group by sex\n",
        "print(\"Fairness Metrics by Sex:\")\n",
        "sex_groups = X_test_compas_copy['sex'].unique()\n",
        "metrics_by_sex = {}\n",
        "for sex in sex_groups:\n",
        "    metrics_by_sex[sex] = compute_metrics(X_test_compas_copy, 'sex', sex)\n",
        "    print(f\"{sex}: {metrics_by_sex[sex]}\")\n",
        "\n",
        "#Compute metrics for each group by race\n",
        "print(\"\\nFairness Metrics by Race:\")\n",
        "race_groups = X_test_compas_copy['race'].unique()\n",
        "metrics_by_race = {}\n",
        "for race in race_groups:\n",
        "    metrics_by_race[race] = compute_metrics(X_test_compas_copy, 'race', race)\n",
        "    print(f\"{race}: {metrics_by_race[race]}\")\n",
        "\n",
        "# Additional Analysis\n",
        "# The proportion of false positives by race:\n",
        "fp_by_race = X_test_compas_copy[(X_test_compas_copy['true'] == 0) & (X_test_compas_copy['pred'] == 1)].race.value_counts(normalize=True)\n",
        "fn_by_race = X_test_compas_copy[(X_test_compas_copy['true'] == 1) & (X_test_compas_copy['pred'] == 0)].race.value_counts(normalize=True)\n",
        "\n",
        "print(\"\\nFalse Positives by Race:\")\n",
        "print(fp_by_race)\n",
        "print(\"\\nFalse Negatives by Race :\")\n",
        "print(fn_by_race)\n",
        "\n",
        "# Similarly, for sex:\n",
        "fp_by_sex = X_test_compas_copy[(X_test_compas_copy['true'] == 0) & (X_test_compas_copy['pred'] == 1)].sex.value_counts(normalize=True)\n",
        "fn_by_sex = X_test_compas_copy[(X_test_compas_copy['true'] == 1) & (X_test_compas_copy['pred'] == 0)].sex.value_counts(normalize=True)\n",
        "\n",
        "print(\"\\nFalse Positives by Sex :\")\n",
        "print(fp_by_sex)\n",
        "print(\"\\nFalse Negatives by Sex :\")\n",
        "print(fn_by_sex)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMH0SdG2ANsJ"
      },
      "outputs": [],
      "source": [
        "#SHAP value for COMPAS_dataset\n",
        "# Sample background data from training set\n",
        "background = X_train_compas.sample(30, random_state=42)\n",
        "\n",
        "# Initialize TreeExplainer with background data\n",
        "explainer = shap.TreeExplainer(model_rf, data=background, model_output='probability')\n",
        "shap_values = explainer.shap_values(X_test_compas)\n",
        "\n",
        "# Extract the SHAP values for class 1 (Recidivism) for all samples\n",
        "shap_values_class1 = np.array([sv[:, 1] for sv in shap_values])\n",
        "print(\"SHAP values for class 1 shape:\", shap_values_class1.shape)\n",
        "\n",
        "# Convert the extracted SHAP values to a dataframe with the same columns as X_test_compas\n",
        "shap_df_compas= pd.DataFrame(shap_values_class1, columns=X_test_compas.columns)\n",
        "shap_df_compas['sample_id'] = shap_df_compas.index\n",
        "\n",
        "# Melt the DataFrame into long format for plotting\n",
        "shap_melted_compas= shap_df_compas.melt(id_vars='sample_id', var_name='feature', value_name='shap_value')\n",
        "\n",
        "# Plot a bar plot of the SHAP values for each feature\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.barplot(x='feature', y='shap_value', data=shap_melted_compas, ci=None)\n",
        "\n",
        "plt.title('Average SHAP Value per Feature (Class 1: Recidivism) COMPAS', fontsize=14)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Average SHAP Value', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xw7dxzYAQJb"
      },
      "outputs": [],
      "source": [
        "#Use LIME Explainer\n",
        "import random\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Create an instance\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=np.array(X_train_compas),\n",
        "    feature_names=X_train_compas.columns.tolist(),\n",
        "    class_names=['No Recidivism', 'Recidivism'],\n",
        "    mode='classification'\n",
        ")\n",
        "#List to collect data\n",
        "lime_data=[]\n",
        "# Loop through 30 random samples from X_test_compas\n",
        "for i in random.sample(range(len(X_test_compas)), 30):\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=X_test_compas.iloc[i],\n",
        "        predict_fn=model_rf.predict_proba,\n",
        "        num_features=5\n",
        "    )\n",
        "    for feature,weight in exp.as_list():\n",
        "        lime_data.append([i,feature,weight])\n",
        "\n",
        "# Convert to DataFrame\n",
        "lime_df = pd.DataFrame(lime_data, columns=['sample_id', 'feature', 'lime_weight'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjIlBkgvASv1"
      },
      "outputs": [],
      "source": [
        "#Make bar plot of Average LIME Weights(since we want to average it rather than making graaph for each local model)\n",
        "#to visualize calculate the average LIME weight per feature\n",
        "avg_weights_compas=lime_df.groupby('feature')['lime_weight'].mean().reset_index()\n",
        "# sort by weight\n",
        "avg_weights_compas=avg_weights_compas.sort_values(by='lime_weight',ascending=False)\n",
        "#plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x='feature',y='lime_weight',data=avg_weights_compas)\n",
        "plt.title('Average LIME Weight by Feature COMPAS', fontsize=14)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Average LIME Weight', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u42RVKl4AVkB"
      },
      "outputs": [],
      "source": [
        "# 4. Reweighing for Class Imbalance and Retraining\n",
        "\n",
        "# Calculate sample weights for the training data to help with class imbalance.\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train_compas)\n",
        "\n",
        "# Initialize and train a Random Forest classifier with reweighing\n",
        "rf_reweighted_compas = RandomForestClassifier(max_depth=3, random_state=42)\n",
        "rf_reweighted_compas.fit(X_train_compas, Y_train_compas, sample_weight=sample_weights)\n",
        "\n",
        "# Predict on the test set with the reweighted model\n",
        "y_pred_rw_compas = rf_reweighted_compas.predict(X_test_compas)\n",
        "\n",
        "print(\"\\nReweighted Random Forest Model Performance:\")\n",
        "print(classification_report(Y_test_compas, y_pred_rw_compas))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test_compas, y_pred_rw_compas))\n",
        "\n",
        "# Optionally, you can re-compute fairness metrics for the reweighted model:\n",
        "X_test_compas_copy['pred'] = y_pred_rw_compas\n",
        "\n",
        "print(\"\\nFairness Metrics by Sex (Reweighted Model):\")\n",
        "metrics_by_sex_rw = {}\n",
        "for sex in X_test_compas_copy['sex'].unique():\n",
        "    metrics_by_sex_rw[sex] = compute_metrics(X_test_compas_copy, 'sex', sex)\n",
        "    print(f\"{sex}: {metrics_by_sex_rw[sex]}\")\n",
        "\n",
        "print(\"\\nFairness Metrics by Race (Reweighted Model):\")\n",
        "metrics_by_race_rw = {}\n",
        "for race in X_test_compas_copy['race'].unique():\n",
        "    metrics_by_race_rw[race] = compute_metrics(X_test_compas_copy, 'race', race)\n",
        "    print(f\"{race}: {metrics_by_race_rw[race]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVQ1coEeAeh_"
      },
      "outputs": [],
      "source": [
        "#Reweighing the class to improve the class imbalance for better prediction\n",
        "# Calculate sample weights for class balance\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train_compas)\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf_reweighted_compas = RandomForestClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Fit with reweighting\n",
        "rf_reweighted_compas.fit(X_train_compas, Y_train_compas, sample_weight=sample_weights)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred_rw_compas = rf_reweighted_compas.predict(X_test_compas)\n",
        "print(\"Reweighted Random Forest Model:\")\n",
        "print(classification_report(Y_test_compas, y_pred_rw_compas))\n",
        "print(confusion_matrix(Y_test_compas, y_pred_rw_compas))\n",
        "#Load the reweighted model into a file\n",
        "joblib.dump(rf_reweighted_compas,'rf_reweighted_compas.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFAHPI-VAgp3"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# SHAP for reweighing\n",
        "# Use background data comes from dataset as the test data\n",
        "background_compas = X_train_compas.sample(30, random_state=42).astype(float)\n",
        "X_test_compas_float = X_test_compas.astype(float)\n",
        "\n",
        "# SHAP Explainer\n",
        "\n",
        "explainer = shap.Explainer(rf_reweighted_compas, background_compas)\n",
        "shap_values = explainer(X_test_compas_float)\n",
        "\n",
        "# Extract SHAP values for class 1 (index 1)\n",
        "shap_array = shap_values.values[:, :, 1]  # Shape: [707, 8]\n",
        "\n",
        "# Create wide-format DataFrame\n",
        "shap_df_compas = pd.DataFrame(shap_array, columns=X_test_compas.columns)\n",
        "\n",
        "# Add sample IDs for reference\n",
        "shap_df_compas['sample_id'] = shap_df_compas.index\n",
        "\n",
        "# Save wide format CSV(this can be used with Phi-2)\n",
        "shap_df_compas.to_csv(\"shap_values_compas_wide.csv\", index=False)\n",
        "print(\"SHAP values saved (wide format) to shap_values_compas_wide.csv\")\n",
        "\n",
        "# Melt to long format for bar plot or analysis\n",
        "shap_melted_compas = shap_df_compas.melt(id_vars='sample_id', var_name='feature', value_name='shap_value')\n",
        "\n",
        "# Save melted format CSV\n",
        "shap_melted_compas.to_csv(\"shap_values_compas_long.csv\", index=False)\n",
        "print(\"SHAP values saved (long format) to shap_values_compas_long.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogy9yoyEAjMy"
      },
      "outputs": [],
      "source": [
        "# Plot a bar plot of the SHAP values for each feature\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.barplot(x='feature', y='shap_value', data=shap_melted_compas, ci=None)\n",
        "\n",
        "plt.title('Average SHAP Value per Feature (Class 1: Recidivism) COMPAS after Reweighing', fontsize=14)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Average SHAP Value', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr7S_MPYAlqZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "# Create an instance for LIME Explainer\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=np.array(X_train_compas),\n",
        "    feature_names=X_train_compas.columns.tolist(),\n",
        "    class_names=['No Recidivism', 'Recidivism'],\n",
        "    mode='classification'\n",
        ")\n",
        "#List to collect data\n",
        "lime_data=[]\n",
        "# Loop through 30 random samples from X_test_compas\n",
        "for i in random.sample(range(len(X_test_compas)), 30):\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=X_test_compas.iloc[i],\n",
        "        predict_fn=model_rf.predict_proba,\n",
        "        num_features=5\n",
        "    )\n",
        "    for feature,weight in exp.as_list():\n",
        "        lime_data.append([i,feature,weight])\n",
        "\n",
        "# Convert to DataFrame\n",
        "lime_df = pd.DataFrame(lime_data, columns=['sample_id', 'feature', 'lime_weight'])\n",
        "lime_df.to_csv(\"lime_values_compas.csv\", index=False)\n",
        "print(\"LIME values saved to lime_values_compas.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diFkzP6ZAnIs"
      },
      "outputs": [],
      "source": [
        "#Make bar plot of Average LIME Weights for COMPAS\n",
        "#calculate the average LIME weight per feature\n",
        "avg_weights_compas=lime_df.groupby('feature')['lime_weight'].mean().reset_index()\n",
        "# sort by weight\n",
        "avg_weights_compas=avg_weights_compas.sort_values(by='lime_weight',ascending=False)\n",
        "#plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x='feature',y='lime_weight',data=avg_weights_compas)\n",
        "plt.title('Average LIME Weight by Feature COMPAS after Reweighing', fontsize=14)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Average LIME Weight', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98DWdK37AvF4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "# Do 5 repeated splits\n",
        "n_splits = 5\n",
        "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lists to store TPR values across sensitive split\n",
        "tpr_baseline_female_list = []\n",
        "tpr_reweighted_female_list = []\n",
        "tpr_baseline_African_American_list = []\n",
        "tpr_reweighted_African_American_list = []\n",
        "\n",
        "# Function to compute TPR\n",
        "def compute_tpr_for_group(model, df, sensitive_col, sensitive_value):\n",
        "    # Make predictions using features only\n",
        "    y_pred = model.predict(df.drop(columns=['two_year_recid', 'race', 'sex']))\n",
        "    df_copy = df.copy()\n",
        "    df_copy['pred'] = y_pred\n",
        "\n",
        "    group = df_copy[df_copy[sensitive_col] == sensitive_value]\n",
        "    TP = ((group['two_year_recid'] == 1) & (group['pred'] == 1)).sum()\n",
        "    FN = ((group['two_year_recid'] == 1) & (group['pred'] == 0)).sum()\n",
        "    return TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
        "\n",
        "for train_idx, test_idx in sss.split(df_compas_encoded, df_compas_encoded['two_year_recid']):\n",
        "    df_train = df_compas_encoded.iloc[train_idx]\n",
        "    df_test = df_compas_encoded.iloc[test_idx]\n",
        "\n",
        "    # Extract features and target for training and testing\n",
        "    X_train = df_train.drop(columns=['two_year_recid', 'race', 'sex'])\n",
        "    y_train = df_train['two_year_recid']\n",
        "    X_test = df_test.drop(columns=['two_year_recid', 'race', 'sex'])\n",
        "    y_test = df_test['two_year_recid']\n",
        "\n",
        "    # Baseline model\n",
        "    model_baseline = RandomForestClassifier(random_state=42)\n",
        "    model_baseline.fit(X_train, y_train)\n",
        "\n",
        "    # Reweighted model (using sample weights)\n",
        "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "    model_reweighted = RandomForestClassifier(max_depth=3, random_state=42)\n",
        "    model_reweighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "    # Create a temporary test DataFrame including sensitive attributes and true labels\n",
        "    df_test_temp = X_test.copy()\n",
        "    df_test_temp['two_year_recid'] = y_test\n",
        "    df_test_temp['race'] = df_test['race']\n",
        "    df_test_temp['sex'] = df_test['sex']\n",
        "\n",
        "    # Compute TPR for the \"Female\" group\n",
        "    tpr_base_sex = compute_tpr_for_group(model_baseline, df_test_temp, sensitive_col=\"sex\", sensitive_value=\"Female\")\n",
        "    tpr_rew_sex = compute_tpr_for_group(model_reweighted, df_test_temp, sensitive_col=\"sex\", sensitive_value=\"Female\")\n",
        "\n",
        "    tpr_baseline_female_list.append(tpr_base_sex)\n",
        "    tpr_reweighted_female_list.append(tpr_rew_sex)\n",
        "\n",
        "    # Compute TPR for the \"African-American\" group\n",
        "    tpr_base_race = compute_tpr_for_group(model_baseline, df_test_temp, sensitive_col=\"race\", sensitive_value=\"African-American\")\n",
        "    tpr_rew_race = compute_tpr_for_group(model_reweighted, df_test_temp, sensitive_col=\"race\", sensitive_value=\"African-American\")\n",
        "\n",
        "    tpr_baseline_African_American_list.append(tpr_base_race)\n",
        "    tpr_reweighted_African_American_list.append(tpr_rew_race)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "tpr_baseline_female = np.array(tpr_baseline_female_list)\n",
        "tpr_reweighted_female = np.array(tpr_reweighted_female_list)\n",
        "tpr_baseline_African_American = np.array(tpr_baseline_African_American_list)\n",
        "tpr_reweighted_African_American = np.array(tpr_reweighted_African_American_list)\n",
        "\n",
        "# Paired t-test for the Female group\n",
        "t_stat_female, p_value_female = ttest_rel(tpr_reweighted_female, tpr_baseline_female)\n",
        "print(\"Paired t-test for TPR in Female group:\")\n",
        "print(\"t-statistic: {:.3f}, p-value: {:.3f}\".format(t_stat_female, p_value_female))\n",
        "if p_value_female < 0.05:\n",
        "    print(\"The improvement in TPR for females is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"No statistically significant improvement in TPR for females (p >= 0.05).\")\n",
        "\n",
        "# Paired t-test for the African-American group\n",
        "t_stat_race, p_value_race = ttest_rel(tpr_reweighted_African_American, tpr_baseline_African_American)\n",
        "print(\"\\nPaired t-test for TPR in African-American group:\")\n",
        "print(\"t-statistic: {:.3f}, p-value: {:.3f}\".format(t_stat_race, p_value_race))\n",
        "if p_value_race < 0.05:\n",
        "    print(\"The improvement in TPR for African-American group is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"No statistically significant improvement in TPR for African-American group (p >= 0.05).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPCXCGiRA1sC"
      },
      "outputs": [],
      "source": [
        "#Apply SLM to reduce bias and also explain both prediction and bias after training in word\n",
        "# Load tokenizer and phi-2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5aJPCteA7ER"
      },
      "outputs": [],
      "source": [
        "# Check unique features\n",
        "print(\"Unique features in the file:\", shap_melted_compas[\"feature\"].unique())\n",
        "\n",
        "# Adjust filter based on actual labels:\n",
        "filtered_shap = shap_melted_compas[shap_melted_compas[\"feature\"].isin([\"Race\", \"Sex\"])]\n",
        "\n",
        "# Confirm that filtering returned rows\n",
        "print(\"Filtered SHAP DataFrame shape:\", filtered_shap.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2cQE4wBfA9XK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_shap_prompt(feature, shap_value, prediction_label, dataset_name):\n",
        "\n",
        "    return (\n",
        "        f\"In the {dataset_name} dataset, the feature '{feature}' had a SHAP value of {shap_value:.2f}, \"\n",
        "        f\"and the model predicted '{prediction_label}'. Explain in simple terms how this feature influenced the prediction, \"\n",
        "        f\"and discuss whether it might reflect bias.\\nExplanation:\"\n",
        "    )\n",
        "\n",
        "def process_top_shap_features(df, dataset_name, output_csv_name, tokenizer, model, top_n=1):\n",
        "\n",
        "    explanations = []\n",
        "\n",
        "    # Group the data by instance_id\n",
        "    grouped = df.groupby(\"instance_id\")\n",
        "\n",
        "    for instance_id, group in grouped:\n",
        "        # Select top_n features based on absolute SHAP value\n",
        "        top_features = group.reindex(group[\"shap_value\"].abs().sort_values(ascending=False).index)[:top_n]\n",
        "        for idx, row in top_features.iterrows():\n",
        "            try:\n",
        "                prompt = generate_shap_prompt(\n",
        "                    row[\"feature\"], row[\"shap_value\"], row[\"prediction_label\"], dataset_name\n",
        "                )\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=150,\n",
        "                    temperature=0.4,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True\n",
        "                )\n",
        "                full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                explanation_only = full_output[len(prompt):].strip()\n",
        "\n",
        "                explanations.append({\n",
        "                    \"instance_id\": instance_id,\n",
        "                    \"feature\": row[\"feature\"],\n",
        "                    \"shap_value\": row[\"shap_value\"],\n",
        "                    \"prediction_label\": row[\"prediction_label\"],\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"explanation\": explanation_only\n",
        "                })\n",
        "\n",
        "                print(f\"[{dataset_name}] Processed instance {instance_id} for feature '{row['feature']}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing instance {instance_id}, feature {row['feature']}: {e}\")\n",
        "\n",
        "    # Save the generated explanations to CSV\n",
        "    results_df = pd.DataFrame(explanations)\n",
        "    results_df.to_csv(output_csv_name, index=False)\n",
        "    print(f\"All top SHAP explanations saved to {output_csv_name}\")\n",
        "\n",
        "\n",
        "# Load the CSV file with top SHAP features\n",
        "shap_compas = pd.read_csv(\"shap_values_compas_long.csv\")\n",
        "shap_compas=shap_melted_compas.rename(columns={\"sample_id\": \"instance_id\"})\n",
        "# Define possible labels\n",
        "possible_labels = [\"Recidivism\", \"No Recidivism\"]\n",
        "\n",
        "# Assign a random label from the possible labels\n",
        "shap_compas[\"prediction_label\"] = np.random.choice(possible_labels, size=len(shap_compas))\n",
        "\n",
        "# Create an instance_id column if it doesn't exist\n",
        "if \"instance_id\" not in shap_compas.columns:\n",
        "    shap_compas[\"instance_id\"] = shap_compas.index\n",
        "\n",
        "\n",
        "\n",
        "# Process the entire SHAP DataFrame to generate explanations\n",
        "process_top_shap_features(\n",
        "    df=shap_compas.head(30),\n",
        "    dataset_name=\"COMPAS\",\n",
        "    output_csv_name=\"shap_values_compas_all.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5zFpMXlA_y4"
      },
      "outputs": [],
      "source": [
        "print(\"Unique features:\", shap_compas[\"feature\"].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk2jqJg8BCzV"
      },
      "outputs": [],
      "source": [
        "# Define the sensitive features you want to focus on\n",
        "sensitive_features = [\"race_encoded\", \"sex_encoded\"]\n",
        "\n",
        "# Filter the DataFrame to include only rows where the feature is one of the sensitive features\n",
        "sensitive_shap = shap_compas[shap_compas[\"feature\"].isin(sensitive_features)].copy()\n",
        "\n",
        "# Define your possible labels for binary outcomes\n",
        "possible_labels = [\"Recidivism\", \"No Recidivism\"]\n",
        "\n",
        "# Assign a random label from the possible labels to each row in the filtered DataFrame\n",
        "sensitive_shap[\"prediction_label\"] = np.random.choice(possible_labels, size=len(sensitive_shap))\n",
        "\n",
        "# Process the filtered dataFrame to generate natural language explanations for sensitive features\n",
        "process_top_shap_features(\n",
        "    df=sensitive_shap.head(10),\n",
        "    dataset_name=\"COMPAS\",\n",
        "    output_csv_name=\"shap_values_compas_sensitive.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSRRI7mhBD5p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def generate_lime_prompt(feature, impact, prediction_label, dataset_name):\n",
        "\n",
        "    return (\n",
        "        f\"In the {dataset_name} dataset, the feature '{feature}' had an impact score of {impact:.2f} \"\n",
        "        f\"on the prediction '{prediction_label}'. Explain in simple terms how this feature influenced the prediction, \"\n",
        "        f\"and discuss any potential bias associated with it.\\nExplanation:\"\n",
        "    )\n",
        "\n",
        "def process_top_lime_features(df, dataset_name, output_csv_name, tokenizer, model, top_n=1):\n",
        "\n",
        "    explanations = []\n",
        "\n",
        "    # Group the data by instance_id\n",
        "    grouped = df.groupby(\"instance_id\")\n",
        "\n",
        "    for instance_id, group in grouped:\n",
        "        # Select top_n features based on absolute impact value\n",
        "        top_features = group.reindex(group[\"impact\"].abs().sort_values(ascending=False).index)[:top_n]\n",
        "        for idx, row in top_features.iterrows():\n",
        "            try:\n",
        "                prompt = generate_lime_prompt(\n",
        "                    row[\"feature\"], row[\"impact\"], row[\"prediction_label\"], dataset_name\n",
        "                )\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=150,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    do_sample=True\n",
        "                )\n",
        "                full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                explanation_only = full_output[len(prompt):].strip()\n",
        "\n",
        "                explanations.append({\n",
        "                    \"instance_id\": instance_id,\n",
        "                    \"feature\": row[\"feature\"],\n",
        "                    \"impact\": row[\"impact\"],\n",
        "                    \"prediction_label\": row[\"prediction_label\"],\n",
        "                    \"dataset\": dataset_name,\n",
        "                    \"explanation\": explanation_only\n",
        "                })\n",
        "\n",
        "                print(f\"[{dataset_name}] Processed instance {instance_id} for feature '{row['feature']}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing instance {instance_id}, feature {row['feature']}: {e}\")\n",
        "\n",
        "    # Save the generated explanations to CSV\n",
        "    results_df = pd.DataFrame(explanations)\n",
        "    results_df.to_csv(output_csv_name, index=False)\n",
        "    print(f\"All top LIME explanations saved to {output_csv_name}\")\n",
        "\n",
        "\n",
        "# Load the CSV file with LIME top features\n",
        "lime_compas = pd.read_csv(\"lime_values_compas.csv\")\n",
        "lime_compas = lime_compas.rename(columns={\"sample_id\": \"instance_id\", \"lime_weight\": \"impact\"})\n",
        "# Rename 'sample_id' to 'instance_id'\n",
        "lime_compas = lime_compas.rename(columns={\"sample_id\": \"instance_id\"})\n",
        "\n",
        "# Add a dummy prediction_label\n",
        "lime_compas[\"prediction_label\"] = \"Recidivism\"\n",
        "\n",
        "# Process a small sample  to generate explanations\n",
        "process_top_lime_features(\n",
        "    df=lime_compas.head(10),\n",
        "    dataset_name=\"COMPAS\",\n",
        "    output_csv_name=\"lime_values_compas_final.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0afycfD4BGhV"
      },
      "outputs": [],
      "source": [
        "#Load the adult dataset\n",
        "df_adult = pd.read_csv('/content/drive/MyDrive/NCUR/adult.csv')\n",
        "df_adult.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UKzSEEwBH0s"
      },
      "outputs": [],
      "source": [
        "#Read the adult.csv file\n",
        "df_adult.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBcaPSEEBLS1"
      },
      "outputs": [],
      "source": [
        "df_adult.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Kpu0qdBNMc"
      },
      "outputs": [],
      "source": [
        "#Drop column\n",
        "columns_to_drop_adult=['fnlwgt','capital-gain','capital-loss','native-country','education','relationship']\n",
        "df_adult=df_adult.drop(columns=columns_to_drop_adult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy2SjhAWBRc3"
      },
      "outputs": [],
      "source": [
        "#Check for duplicates before dropping it\n",
        "df_adult.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV7du6q3BS5r"
      },
      "outputs": [],
      "source": [
        "df_adult.to_csv('cleaned_adult.csv',index=False)\n",
        "df_adult_cleaned = df_adult.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjpKhPQvBU34"
      },
      "outputs": [],
      "source": [
        "df_adult_cleaned.info()\n",
        "df_adult_cleaned.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFB-CWelBXEv"
      },
      "outputs": [],
      "source": [
        "# Check for any '?' entries\n",
        "for col in df_adult_cleaned.columns:\n",
        "    print(f\"{col}: {(df_adult_cleaned[col] == '?').sum()} \")\n",
        "# Count '?' in workclass and occupation by gender\n",
        "for col in ['workclass', 'occupation']:\n",
        "    # Filter the DataFrame for rows where the specified column has '?'\n",
        "    missing_rows = df_adult_cleaned[df_adult_cleaned[col] == '?']\n",
        "    # Get the value counts of 'gender' in the filtered rows\n",
        "    missing_by_gender = missing_rows['gender'].value_counts()\n",
        "    print(f\"\\nMissing '{col}' entries by gender:\\n{missing_by_gender}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1ZP0wOJBYxD"
      },
      "outputs": [],
      "source": [
        "#Drop the workclass and occupation columns since the difference is negligible in gender\n",
        "columns_to_drop_adult=['workclass','occupation']\n",
        "df_adult_cleaned=df_adult_cleaned.drop(columns=columns_to_drop_adult)\n",
        "print(df_adult_cleaned.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEJNn_EGBa28"
      },
      "outputs": [],
      "source": [
        "#Encoding the adult dataset next\n",
        "#Make a copy of the cleaned adult\n",
        "df_adult_cleaned=df_adult_cleaned.copy()\n",
        "\n",
        "#map the encoding to use fairness analysis\n",
        "\n",
        "gender_mapping={'Female':0,'Male':1}\n",
        "income_mapping={'<=50K':0,'>50K':1}\n",
        "\n",
        "race_mapping_adult = {\n",
        "    'Black': 0,\n",
        "    'White': 1,\n",
        "    'Asian-Pac-Islander': 2,\n",
        "    'Amer-Indian-Eskimo': 3,\n",
        "    'Other': 4\n",
        "}\n",
        "\n",
        "df_adult_cleaned['gender_encoded']=df_adult_cleaned['gender'].map(gender_mapping)\n",
        "df_adult_cleaned['income_encoded']=df_adult_cleaned['income'].map(income_mapping)\n",
        "df_adult_cleaned['race_encoded']=df_adult_cleaned['race'].map(race_mapping_adult)\n",
        "\n",
        "#One-Hot Encoding multi-class categorical columns\n",
        "df_adult_cleaned=pd.get_dummies(df_adult_cleaned,columns=['marital-status'])\n",
        "\n",
        "df_adult_cleaned.to_csv('adult_encoded.csv', index=False)\n",
        "df_adult_encoded = df_adult_cleaned.copy()\n",
        "df_adult_encoded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyGez4jBCaSY"
      },
      "outputs": [],
      "source": [
        "#Check if they are actually encoded or not\n",
        "df_adult_encoded['gender_encoded'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLzwfNaGCcrt"
      },
      "outputs": [],
      "source": [
        "df_adult_encoded['income_encoded'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aJcFmiECe_3"
      },
      "outputs": [],
      "source": [
        "df_adult_encoded['race_encoded'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFqGn8MWCgwH"
      },
      "outputs": [],
      "source": [
        "#Split the dataset into to train and test split then train\n",
        "#Perform logisitc regression on adult dataset\n",
        "X_adult=df_adult_encoded.drop(columns=['income','race','gender','income_encoded'])\n",
        "Y_adult=df_adult_encoded['income_encoded']\n",
        "X_train_adult,X_test_adult,Y_train_adult,Y_test_adult=train_test_split(X_adult,Y_adult,test_size=0.2,random_state=42)\n",
        "model_lr=LogisticRegression(max_iter=1000)\n",
        "model_lr.fit(X_train_adult,Y_train_adult)\n",
        "y_pred_lg=model_lr.predict(X_test_adult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4-6VWgvCjKi"
      },
      "outputs": [],
      "source": [
        "#Evaluate the performance\n",
        "print(classification_report(Y_test_adult,y_pred_lg))\n",
        "print(confusion_matrix(Y_test_adult,y_pred_lg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qb9tVuCCkzO"
      },
      "outputs": [],
      "source": [
        "#Train the model with XGBoost Classifier\n",
        "X_adult=df_adult_encoded.drop(columns=['income','race','gender','income_encoded'])\n",
        "Y_adult=df_adult_encoded['income_encoded']\n",
        "X_train_adult,X_test_adult,Y_train_adult,Y_test_adult=train_test_split(X_adult,Y_adult,test_size=0.2,random_state=42)\n",
        "model_xg=XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, random_state=42)\n",
        "model_xg.fit(X_train_adult,Y_train_adult)\n",
        "y_pred_xg=model_xg.predict(X_test_adult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxTkJoVpCmNI"
      },
      "outputs": [],
      "source": [
        "#Evaluate the performance fo xg booster\n",
        "print(classification_report(Y_test_adult,y_pred_xg))\n",
        "print(confusion_matrix(Y_test_adult,y_pred_xg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NOIwlLMCnkU"
      },
      "outputs": [],
      "source": [
        "#Apply same thing for the adult dataset\n",
        "import pandas as pd\n",
        "\n",
        "def compute_metrics(df, sensitive_column, sensitive_value):\n",
        "    \"\"\"\n",
        "    Compute fairness metrics for a subgroup.\n",
        "    \"\"\"\n",
        "    group = df[df[sensitive_column] == sensitive_value]\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    TP = ((group['true'] == 1) & (group['pred'] == 1)).sum()\n",
        "    TN = ((group['true'] == 0) & (group['pred'] == 0)).sum()\n",
        "    FP = ((group['true'] == 0) & (group['pred'] == 1)).sum()\n",
        "    FN = ((group['true'] == 1) & (group['pred'] == 0)).sum()\n",
        "\n",
        "    # Calculate metrics with checks to avoid division by zero\n",
        "    fpr = FP / (FP + TN) if (FP + TN) > 0 else None\n",
        "    fnr = FN / (FN + TP) if (FN + TP) > 0 else None\n",
        "    tnr = TN / (TN + FP) if (TN + FP) > 0 else None\n",
        "    tpr = TP / (TP + FN) if (TP + FN) > 0 else None\n",
        "\n",
        "    return {'FPR': fpr, 'FNR': fnr, 'TNR': tnr, 'TPR': tpr}\n",
        "\n",
        "# Prepare the Adult test set with sensitive attributes\n",
        "\n",
        "# Create a copy of the test features\n",
        "X_test_adult_copy = X_test_adult.copy()\n",
        "\n",
        "# Attach the true labels and model predictions\n",
        "X_test_adult_copy['true'] = Y_test_adult\n",
        "X_test_adult_copy['pred'] = y_pred_xg\n",
        "\n",
        "# Re-attach sensitive attributes\n",
        "X_test_adult_copy['race'] = df_adult_cleaned.loc[X_test_adult.index, 'race']\n",
        "X_test_adult_copy['gender'] = df_adult_cleaned.loc[X_test_adult.index, 'gender']\n",
        "\n",
        "# Compute fairness metrics for each subgroup by gender\n",
        "print(\"Fairness Metrics by Gender:\")\n",
        "gender_groups = X_test_adult_copy['gender'].unique()\n",
        "metrics_by_gender = {}\n",
        "for gender in gender_groups:\n",
        "    metrics_by_gender[gender] = compute_metrics(X_test_adult_copy, 'gender', gender)\n",
        "    print(f\"{gender}: {metrics_by_gender[gender]}\")\n",
        "\n",
        "# Compute fairness metrics for each subgroup by race\n",
        "print(\"\\nFairness Metrics by Race:\")\n",
        "race_groups = X_test_adult_copy['race'].unique()\n",
        "metrics_by_race = {}\n",
        "for race in race_groups:\n",
        "    metrics_by_race[race] = compute_metrics(X_test_adult_copy, 'race', race)\n",
        "    print(f\"{race}: {metrics_by_race[race]}\")\n",
        "\n",
        "# Additional Analysis\n",
        "fp_by_race = X_test_adult_copy[(X_test_adult_copy['true'] == 0) & (X_test_adult_copy['pred'] == 1)]['race'].value_counts(normalize=True)\n",
        "fn_by_race = X_test_adult_copy[(X_test_adult_copy['true'] == 1) & (X_test_adult_copy['pred'] == 0)]['race'].value_counts(normalize=True)\n",
        "\n",
        "print(\"\\nFalse Positives by Race :\")\n",
        "print(fp_by_race)\n",
        "print(\"\\nFalse Negatives by Race:\")\n",
        "print(fn_by_race)\n",
        "\n",
        "# Similarly, for gender:\n",
        "fp_by_gender = X_test_adult_copy[(X_test_adult_copy['true'] == 0) & (X_test_adult_copy['pred'] == 1)]['gender'].value_counts(normalize=True)\n",
        "fn_by_gender = X_test_adult_copy[(X_test_adult_copy['true'] == 1) & (X_test_adult_copy['pred'] == 0)]['gender'].value_counts(normalize=True)\n",
        "\n",
        "print(\"\\nFalse Positives by Gender:\")\n",
        "print(fp_by_gender)\n",
        "print(\"\\nFalse Negatives by Gender:\")\n",
        "print(fn_by_gender)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gewN5Qt5Co10"
      },
      "outputs": [],
      "source": [
        "#Work on the SHAP values for the adult dataset\n",
        "# Sample background data\n",
        "background_adult = X_train_adult.sample(30, random_state=42).astype(float)\n",
        "X_test_adult_float = X_test_adult.astype(float)\n",
        "\n",
        "# Initialize SHAP Explainer\n",
        "explainer = shap.Explainer(model_xg, background_adult)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer(X_test_adult_float)\n",
        "\n",
        "# Extract SHAP values\n",
        "shap_values_class1_adult = shap_values.values\n",
        "\n",
        "# Confirm shapes match\n",
        "print(shap_values_class1_adult.shape)\n",
        "print(X_test_adult.columns.shape)\n",
        "\n",
        "# Convert to DataFrame\n",
        "shap_df_adult = pd.DataFrame(shap_values_class1_adult, columns=X_test_adult.columns)\n",
        "shap_df_adult['sample_id'] = shap_df_adult.index\n",
        "\n",
        "# Melt for seaborn plot so we can visulaize it\n",
        "shap_melted_adult = shap_df_adult.melt(id_vars='sample_id', var_name='feature', value_name='shap_value')\n",
        "# SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test_adult, plot_type=\"bar\", show=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b_xlgb-Cqsl"
      },
      "outputs": [],
      "source": [
        "#Apply the LIME for adult dataset too\n",
        "# Convert to numpy arrays\n",
        "X_train_np = X_train_adult.values\n",
        "X_test_np = X_test_adult.values\n",
        "feature_names = X_train_adult.columns.tolist()\n",
        "class_names = ['<=50K', '>50K']\n",
        "\n",
        "# Initialize LIME Explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_np,\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "# All the LIME weights into a list\n",
        "lime_weights_list = []\n",
        "\n",
        "# Loop over samples from test set\n",
        "N = 30\n",
        "for i in range(N):\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=X_test_np[i],\n",
        "        predict_fn=model_xg.predict_proba,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    # Get explanation\n",
        "    for feature, weight in exp.as_list():\n",
        "        lime_weights_list.append({'feature': feature, 'lime_weight': weight})\n",
        "\n",
        "# Convert to DataFrame\n",
        "lime_df = pd.DataFrame(lime_weights_list)\n",
        "\n",
        "# Group by feature and calculate average weight\n",
        "avg_weights_adult = lime_df.groupby('feature')['lime_weight'].mean().reset_index()\n",
        "avg_weights_adult = avg_weights_adult.sort_values(by='lime_weight', ascending=False)\n",
        "\n",
        "# Plot average LIME weight per feature\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='feature', y='lime_weight', data=avg_weights_adult)\n",
        "plt.title('Average LIME Weight by Feature (Adult Dataset)', fontsize=14)\n",
        "plt.xlabel('Feature', fontsize=12)\n",
        "plt.ylabel('Average LIME Weight', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO5byVKPCsdm"
      },
      "outputs": [],
      "source": [
        "#Reweighing for the ADULT dataset\n",
        "# Calculate sample weights\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=Y_train_adult)\n",
        "\n",
        "# Fit model with sample weights\n",
        "model_xg_reweighted = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, random_state=42)\n",
        "model_xg_reweighted.fit(X_train_adult, Y_train_adult, sample_weight=sample_weights)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred_rw_adult = model_xg_reweighted.predict(X_test_adult)\n",
        "print(\"Reweighted Model for Adult dataset:\")\n",
        "print(classification_report(Y_test_adult, y_pred_rw_adult))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(Y_test_adult, y_pred_rw_adult))\n",
        "\n",
        "# Optionally, you can re-compute fairness metrics for the reweighted model:\n",
        "X_test_adult_copy['pred'] = y_pred_rw_adult\n",
        "\n",
        "print(\"\\nFairness Metrics by Gender (Reweighted Model):\")\n",
        "metrics_by_gender_rw = {}\n",
        "for sex in X_test_adult_copy['gender'].unique():\n",
        "    metrics_by_gender_rw[gender] = compute_metrics(X_test_adult_copy, 'gender', gender)\n",
        "    print(f\"{gender}: {metrics_by_gender_rw[gender]}\")\n",
        "\n",
        "print(\"\\nFairness Metrics by Race (Reweighted Model):\")\n",
        "metrics_by_race_rw = {}\n",
        "for race in X_test_adult_copy['race'].unique():\n",
        "    metrics_by_race_rw[race] = compute_metrics(X_test_adult_copy, 'race', race)\n",
        "    print(f\"{race}: {metrics_by_race_rw[race]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksGi5R7aCudN"
      },
      "outputs": [],
      "source": [
        "#SHAP after reweighing the values for Adult dataset\n",
        "\n",
        "# Background for SHAP\n",
        "background_adult = X_train_adult.sample(50, random_state=42).astype(float)\n",
        "X_test_adult_float = X_test_adult.astype(float)\n",
        "\n",
        "# SHAP Explainer after Reweighing\n",
        "explainer = shap.Explainer(model_xg_reweighted, background_adult)\n",
        "shap_values = explainer(X_test_adult_float)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test_adult, plot_type=\"bar\", show=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_BLSRY2Cwad"
      },
      "outputs": [],
      "source": [
        "#Save reweighted SHAP values in dataframe then csv for later\n",
        "import pandas as pd\n",
        "\n",
        "# Extract SHAP values\n",
        "shap_array = shap_values.values\n",
        "\n",
        "# Convert to DataFrame with feature names\n",
        "shap_df = pd.DataFrame(shap_array, columns=X_test_adult.columns)\n",
        "\n",
        "# Add prediction and true labels for context\n",
        "shap_df[\"prediction\"] = model_xg_reweighted.predict(X_test_adult)\n",
        "shap_df[\"true_label\"] = Y_test_adult.values\n",
        "#Convert it to csv\n",
        "shap_df.to_csv(\"shap_values_adult.csv\", index=False)\n",
        "print(\"SHAP values saved to shap_values_adult.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQKLE2B0CzWv"
      },
      "outputs": [],
      "source": [
        "#LIME after reweighing\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_adult.values,\n",
        "    feature_names=X_train_adult.columns.tolist(),\n",
        "    class_names=['<=50K', '>50K'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "lime_weights = []\n",
        "for i in range(30):\n",
        "    exp = explainer.explain_instance(\n",
        "        data_row=X_test_adult.values[i],\n",
        "        predict_fn=model_xg_reweighted.predict_proba,\n",
        "        num_features=10\n",
        "    )\n",
        "    for feature, weight in exp.as_list():\n",
        "        lime_weights.append({'feature': feature, 'lime_weight': weight})\n",
        "\n",
        "lime_df = pd.DataFrame(lime_weights)\n",
        "avg_weights = lime_df.groupby('feature')['lime_weight'].mean().reset_index()\n",
        "\n",
        "# Plot\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x='feature', y='lime_weight', data=avg_weights.sort_values('lime_weight', ascending=False))\n",
        "plt.title('Average LIME Weight per Feature (Post-Mitigation)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCEwv6LZC0Zt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "\n",
        "# Do 5 repeated splits\n",
        "n_splits = 5\n",
        "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lists to store TPR values across splits\n",
        "tpr_baseline_female_list = []\n",
        "tpr_reweighted_female_list = []\n",
        "tpr_baseline_Black_list = []\n",
        "tpr_reweighted_Black_list = []\n",
        "\n",
        "# Function to compute TPR from a DataFrame\n",
        "def compute_tpr_for_group_adult(model, df, sensitive_col, sensitive_value):\n",
        "    # Make predictions using features only\n",
        "    y_pred = model.predict(df.drop(columns=['income', 'race', 'gender']))\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    df_copy = df.copy()\n",
        "    df_copy['pred'] = y_pred\n",
        "    # Select the subgroup based on the sensitive attribute\n",
        "    group = df_copy[df_copy[sensitive_col] == sensitive_value]\n",
        "\n",
        "\n",
        "    TP = ((group['income'] == 1) & (group['pred'] == 1)).sum()\n",
        "    FN = ((group['income'] == 1) & (group['pred'] == 0)).sum()\n",
        "    return TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
        "\n",
        "# Loop through repeated splits\n",
        "for train_idx, test_idx in sss.split(df_adult_encoded, df_adult_encoded['income']):\n",
        "    df_train = df_adult_encoded.iloc[train_idx]\n",
        "    df_test = df_adult_encoded.iloc[test_idx]\n",
        "\n",
        "    # Extract features and target for training and testing\n",
        "    X_train = df_train.drop(columns=['income', 'race', 'gender'])\n",
        "    y_train = df_train['income_encoded']  # Assuming this is the numeric encoding of income\n",
        "    X_test = df_test.drop(columns=['income', 'race', 'gender'])\n",
        "    y_test = df_test['income_encoded']\n",
        "\n",
        "    # Baseline model training\n",
        "    model_baseline = XGBClassifier(random_state=42)\n",
        "    model_baseline.fit(X_train, y_train)\n",
        "\n",
        "    # Reweighted model training using sample weights\n",
        "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "    model_reweighted = XGBClassifier(max_depth=3, random_state=42)\n",
        "    model_reweighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "    # Create a temporary test DataFrame including sensitive attributes and true labels\n",
        "    df_test_temp = X_test.copy()\n",
        "    df_test_temp['income'] = y_test  # Note: this column is used as the true label\n",
        "    df_test_temp['race'] = df_test['race']\n",
        "    df_test_temp['gender'] = df_test['gender']\n",
        "\n",
        "    # Compute TPR for the \"Female\" group\n",
        "    tpr_base_gender = compute_tpr_for_group_adult(model_baseline, df_test_temp, sensitive_col=\"gender\", sensitive_value=\"Female\")\n",
        "    tpr_rew_gender = compute_tpr_for_group_adult(model_reweighted, df_test_temp, sensitive_col=\"gender\", sensitive_value=\"Female\")\n",
        "\n",
        "    tpr_baseline_female_list.append(tpr_base_gender)\n",
        "    tpr_reweighted_female_list.append(tpr_rew_gender)\n",
        "\n",
        "    # Compute TPR for the \"Black\" group\n",
        "    tpr_base_race = compute_tpr_for_group_adult(model_baseline, df_test_temp, sensitive_col=\"race\", sensitive_value=\"Black\")\n",
        "    tpr_rew_race = compute_tpr_for_group_adult(model_reweighted, df_test_temp, sensitive_col=\"race\", sensitive_value=\"Black\")\n",
        "\n",
        "    tpr_baseline_Black_list.append(tpr_base_race)\n",
        "    tpr_reweighted_Black_list.append(tpr_rew_race)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "tpr_baseline_female = np.array(tpr_baseline_female_list)\n",
        "tpr_reweighted_female = np.array(tpr_reweighted_female_list)\n",
        "tpr_baseline_Black = np.array(tpr_baseline_Black_list)\n",
        "tpr_reweighted_Black = np.array(tpr_reweighted_Black_list)\n",
        "\n",
        "# Paired t-test for the Female group\n",
        "t_stat_female, p_value_female = ttest_rel(tpr_reweighted_female, tpr_baseline_female)\n",
        "print(\"Paired t-test for TPR in Female group:\")\n",
        "print(\"t-statistic: {:.3f}, p-value: {:.3f}\".format(t_stat_female, p_value_female))\n",
        "if p_value_female < 0.05:\n",
        "    print(\"The improvement in TPR for females is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"No statistically significant improvement in TPR for females (p >= 0.05).\")\n",
        "\n",
        "# Paired t-test for the Black group\n",
        "t_stat_race, p_value_race = ttest_rel(tpr_reweighted_Black, tpr_baseline_Black)\n",
        "print(\"\\nPaired t-test for TPR in Black group:\")\n",
        "print(\"t-statistic: {:.3f}, p-value: {:.3f}\".format(t_stat_race, p_value_race))\n",
        "if p_value_race < 0.05:\n",
        "    print(\"The improvement in TPR for Black group is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"No statistically significant improvement in TPR for Black group (p >= 0.05).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kImpCavNC27Y"
      },
      "outputs": [],
      "source": [
        "print(\"Columns in shap_adult:\", shap_df_adult.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2m-gYxJC55P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your wide-format CSV file\n",
        "shap_adult = pd.read_csv(\"shap_values_adult.csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Rename 'sample_id' to 'instance_id' if it exists\n",
        "if \"sample_id\" in shap_adult.columns:\n",
        "    shap_adult = shap_adult.rename(columns={\"sample_id\": \"instance_id\"})\n",
        "\n",
        "# If 'instance_id' still does not exist, create it from the index\n",
        "if \"instance_id\" not in shap_adult.columns:\n",
        "    shap_adult[\"instance_id\"] = shap_adult.index\n",
        "\n",
        "# Assign random prediction labels for testing:\n",
        "possible_labels = [\"Income>=50k\", \"Income<=50k\"]\n",
        "shap_adult[\"prediction_label\"] = np.random.choice(possible_labels, size=len(shap_adult))\n",
        "\n",
        "# Define id_vars for the melt operation.\n",
        "id_vars = [\"instance_id\", \"prediction\", \"true_label\", \"prediction_label\"]\n",
        "\n",
        "# Identify feature columns by excluding id_vars.\n",
        "feature_columns = [col for col in shap_adult.columns if col not in id_vars]\n",
        "\n",
        "# Melt the DataFrame: this creates two new columns \"feature\" and \"shap_value\"\n",
        "melted_shap_adult = shap_adult.melt(\n",
        "    id_vars=[\"instance_id\", \"prediction_label\"],\n",
        "    value_vars=feature_columns,\n",
        "    var_name=\"feature\",\n",
        "    value_name=\"shap_value\"\n",
        ")\n",
        "\n",
        "# Check the result\n",
        "print(\"Melted DataFrame columns:\", melted_shap_adult.columns)\n",
        "print(melted_shap_adult.head())\n",
        "\n",
        "# Now you can call your process_top_shap_features function on a subset\n",
        "process_top_shap_features(\n",
        "    df=melted_shap_adult.head(30),\n",
        "    dataset_name=\"Adult\",\n",
        "    output_csv_name=\"shap_values_adult_all.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CapEhnMNC71-"
      },
      "outputs": [],
      "source": [
        "#Do the same generation for sensitive dataset\n",
        "# Define the sensitive features you want to focus on\n",
        "sensitive_features = [\"race_encoded\", \"gender_encoded\"]\n",
        "\n",
        "# Filter the DataFrame to include only rows where the feature is one of the sensitive features\n",
        "sensitive_shap = shap_compas[shap_compas[\"feature\"].isin(sensitive_features)].copy()\n",
        "\n",
        "# Define your possible labels for binary outcomes\n",
        "possible_labels = [\"Income>=50k\", \"Income<=50k\"]\n",
        "\n",
        "# Assign a random label from the possible labels to each row in the filtered DataFrame\n",
        "sensitive_shap[\"prediction_label\"] = np.random.choice(possible_labels, size=len(sensitive_shap))\n",
        "\n",
        "# Process the filtered DataFrame to generate natural language explanations for sensitive features\n",
        "process_top_shap_features(\n",
        "    df=sensitive_shap.head(10),\n",
        "    dataset_name=\"COMPAS\",\n",
        "    output_csv_name=\"shap_values_adult_sensitive.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptCtWUeMC_kh"
      },
      "outputs": [],
      "source": [
        "#Generate for LIME\n",
        "# Load the CSV file with LIME top features\n",
        "lime_compas = pd.read_csv(\"lime_values_compas.csv\")\n",
        "lime_compas = lime_compas.rename(columns={\"sample_id\": \"instance_id\", \"lime_weight\": \"impact\"})\n",
        "# Rename 'sample_id' to 'instance_id'\n",
        "lime_compas = lime_compas.rename(columns={\"sample_id\": \"instance_id\"})\n",
        "\n",
        "\n",
        "# Add a dummy prediction_label\n",
        "\n",
        "# Define your possible labels for binary outcomes\n",
        "possible_labels = [\"Income>=50k\", \"Income<=50k\"]\n",
        "\n",
        "# Assign a random label from the possible labels to each row in the filtered DataFrame\n",
        "lime_compas[\"prediction_label\"] = np.random.choice(possible_labels, size=len(lime_compas))\n",
        "\n",
        "\n",
        "# Process a small sample (first 5 rows) to generate explanations\n",
        "process_top_lime_features(\n",
        "    df=lime_compas.head(10),\n",
        "    dataset_name=\"COMPAS\",\n",
        "    output_csv_name=\"lime_values_compas_final.csv\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_n=3\n",
        ")\n",
        "2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE4E9WmJ9iAWVSRKI+RrME",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}